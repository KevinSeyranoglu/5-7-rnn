{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gorgeous-transparency",
   "metadata": {},
   "source": [
    "# 1. Sentiment analysis\n",
    "\n",
    "Using the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), we want to do a regression model that predict the ratings are on a 1-10 scale. You have an example train and test set in the `dataset` folder.\n",
    "\n",
    "### 1.1 Regression Model\n",
    "\n",
    "Use a feedforward neural network and NLP techniques we've seen up to now to train the best model you can on this dataset\n",
    "\n",
    "### 1.2 RNN model\n",
    "\n",
    "Train a RNN to do the sentiment analysis regression. The RNN should consist simply of an embedding layer (to make word IDs into word vectors) a recurrent blocks (GRU or LSTM) feeding into an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "brave-hamilton",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 24998/24998 [00:05<00:00, 4538.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re \n",
    "\n",
    "# Reading the files, making a dataframe with text and reviews for each rows\n",
    "\n",
    "path_negative='dataset\\\\aclImdb\\\\train\\\\neg'\n",
    "path_positive='dataset\\\\aclImdb\\\\train\\\\pos'\n",
    "\n",
    "onlyfiles_n = [path_negative+'\\\\'+f for f in listdir(path_negative) if isfile(join(path_negative, f))]\n",
    "onlyfiles_p = [path_positive+'\\\\'+f for f in listdir(path_positive) if isfile(join(path_positive, f))]\n",
    "onlyfiles_n=onlyfiles_n[1:]\n",
    "onlyfiles_p=onlyfiles_p[1:]\n",
    "onlyfiles_p.extend(onlyfiles_n)\n",
    "onlyfiles=onlyfiles_p\n",
    "\n",
    "\n",
    "df=pd.DataFrame()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "L=[]\n",
    "rating=[]\n",
    "for File in tqdm(onlyfiles):\n",
    "\n",
    "    #only keeping the review\n",
    "    rating.append(re.sub(\"[^0-9 ]\", \"\", File[-6:]))\n",
    "\n",
    "    f = open(F\"{File}\", encoding=\"utf8\")\n",
    "    a=f.read()\n",
    "    f.close()\n",
    "    #Only keeping letters and numbers and spaces\n",
    "    a=re.sub(\"[^a-zA-Z0-9 ]\", \"\", a)\n",
    "    L.append(a)\n",
    "df=pd.DataFrame([L,rating]).T\n",
    "df.columns=['review','rating']\n",
    "\n",
    "#Shuffle DataFrame rows\n",
    "df=df.sample(frac=0.05).reset_index(drop=True)\n",
    "df.head(5)\n",
    "\n",
    "df.rating=df.rating.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "\n",
    "# Cleaning the data  for the reviews + TF-IDF \n",
    "\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# to remove the stop words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "#preprocess fontion \n",
    "def preprocess(text, stem=False,lem=True):\n",
    "    # Remove link and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            if lem:\n",
    "                tokens.append(lemmatizer.lemmatize(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# cleaning the data for every rows\n",
    "df.review= df.review.apply(lambda x: preprocess(x))\n",
    "\n",
    "\n",
    "tfidf = text.TfidfVectorizer()  \n",
    "tfidf.fit(df.review)\n",
    "t = tfidf.transform(df.review)\n",
    "t=(t.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.1339 - accuracy: 0.0824\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.1339 - accuracy: 0.0928\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.1338 - accuracy: 0.0944\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 12.1338 - accuracy: 0.0952\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 12.1338 - accuracy: 0.0904\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223ebcdbc10>"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Concatenate\n",
    "from keras.models import Model\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.Input(shape=(t.shape[1])))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(t, df.rating, epochs=5, batch_size=125)\n"
   ]
  },
  {
   "source": [
    "## A feedforward neural network does not gice any good results..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RNN model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 review  rating\n",
       "0     moron girlfriend conduct ritual resurrect dead...       1\n",
       "1     pathetic worse bad madefortv movie cant believ...       2\n",
       "2     go see film honestly jim carrey past made hila...       4\n",
       "3     wesley snipe james dial assassin hire agent ci...       4\n",
       "4     good rent buy original watch someone gun head ...       4\n",
       "...                                                 ...     ...\n",
       "1245  movie sit easily even particularly like movie ...       1\n",
       "1246  agree veinbreaker wrote regard ahhhh feeling g...      10\n",
       "1247  series ups occasional latter case agreeable am...       3\n",
       "1248  saw movie advance screening found excellentbr ...      10\n",
       "1249  prerelease version 1933s baby face would make ...       7\n",
       "\n",
       "[1250 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>moron girlfriend conduct ritual resurrect dead...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pathetic worse bad madefortv movie cant believ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>go see film honestly jim carrey past made hila...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wesley snipe james dial assassin hire agent ci...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good rent buy original watch someone gun head ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1245</th>\n      <td>movie sit easily even particularly like movie ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1246</th>\n      <td>agree veinbreaker wrote regard ahhhh feeling g...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1247</th>\n      <td>series ups occasional latter case agreeable am...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1248</th>\n      <td>saw movie advance screening found excellentbr ...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1249</th>\n      <td>prerelease version 1933s baby face would make ...</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>1250 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review  rating  \\\n",
       "0  moron girlfriend conduct ritual resurrect dead...       1   \n",
       "1  pathetic worse bad madefortv movie cant believ...       2   \n",
       "2  go see film honestly jim carrey past made hila...       4   \n",
       "3  wesley snipe james dial assassin hire agent ci...       4   \n",
       "4  good rent buy original watch someone gun head ...       4   \n",
       "5  must accompanied special rating warning recomm...       1   \n",
       "6  remarkable disturbing film truelife senseless ...      10   \n",
       "7  made film love film somebody wacky sense humor...       8   \n",
       "8  inexhaustible hunger basic training movie surp...       8   \n",
       "9  christopher guest need worry supreme hold mock...       2   \n",
       "\n",
       "                                  Tokenized_Sentence  \n",
       "0  [moron, girlfriend, conduct, ritual, resurrect...  \n",
       "1  [pathetic, worse, bad, madefortv, movie, cant,...  \n",
       "2  [go, see, film, honestly, jim, carrey, past, m...  \n",
       "3  [wesley, snipe, james, dial, assassin, hire, a...  \n",
       "4  [good, rent, buy, original, watch, someone, gu...  \n",
       "5  [must, accompanied, special, rating, warning, ...  \n",
       "6  [remarkable, disturbing, film, truelife, sense...  \n",
       "7  [made, film, love, film, somebody, wacky, sens...  \n",
       "8  [inexhaustible, hunger, basic, training, movie...  \n",
       "9  [christopher, guest, need, worry, supreme, hol...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n      <th>Tokenized_Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>moron girlfriend conduct ritual resurrect dead...</td>\n      <td>1</td>\n      <td>[moron, girlfriend, conduct, ritual, resurrect...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pathetic worse bad madefortv movie cant believ...</td>\n      <td>2</td>\n      <td>[pathetic, worse, bad, madefortv, movie, cant,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>go see film honestly jim carrey past made hila...</td>\n      <td>4</td>\n      <td>[go, see, film, honestly, jim, carrey, past, m...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wesley snipe james dial assassin hire agent ci...</td>\n      <td>4</td>\n      <td>[wesley, snipe, james, dial, assassin, hire, a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good rent buy original watch someone gun head ...</td>\n      <td>4</td>\n      <td>[good, rent, buy, original, watch, someone, gu...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>must accompanied special rating warning recomm...</td>\n      <td>1</td>\n      <td>[must, accompanied, special, rating, warning, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>remarkable disturbing film truelife senseless ...</td>\n      <td>10</td>\n      <td>[remarkable, disturbing, film, truelife, sense...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>made film love film somebody wacky sense humor...</td>\n      <td>8</td>\n      <td>[made, film, love, film, somebody, wacky, sens...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>inexhaustible hunger basic training movie surp...</td>\n      <td>8</td>\n      <td>[inexhaustible, hunger, basic, training, movie...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>christopher guest need worry supreme hold mock...</td>\n      <td>2</td>\n      <td>[christopher, guest, need, worry, supreme, hol...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "#Get the word tokens and tags into a readable list format\n",
    "df['Tokenized_Sentence'] = df['review'].apply(lambda sent: sent.split(\" \"))\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WORDS:\nLEXICON SAMPLE (20902 total items):\n{'moron': 2, 'girlfriend': 3, 'conduct': 4, 'ritual': 5, 'resurrect': 6, 'dead': 7, 'attempt': 8, 'prove': 9, 'brought': 10, 'back': 11, 'life': 12, 'surprisingly': 13, 'soul': 14, 'commences': 15, 'chopping': 16, 'axe': 17, 'next': 18, 'day': 19, 'college': 20, 'aged': 21}\n"
     ]
    }
   ],
   "source": [
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    '''Create a lexicon for the words in the sentences as well as the tags'''\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "print(\"WORDS:\")\n",
    "words_lexicon = make_lexicon(df['Tokenized_Sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  Tokenized_Sentence  \\\n",
       "0  [moron, girlfriend, conduct, ritual, resurrect...   \n",
       "1  [pathetic, worse, bad, madefortv, movie, cant,...   \n",
       "2  [go, see, film, honestly, jim, carrey, past, m...   \n",
       "3  [wesley, snipe, james, dial, assassin, hire, a...   \n",
       "4  [good, rent, buy, original, watch, someone, gu...   \n",
       "5  [must, accompanied, special, rating, warning, ...   \n",
       "6  [remarkable, disturbing, film, truelife, sense...   \n",
       "7  [made, film, love, film, somebody, wacky, sens...   \n",
       "8  [inexhaustible, hunger, basic, training, movie...   \n",
       "9  [christopher, guest, need, worry, supreme, hol...   \n",
       "\n",
       "                                       Sentence_Idxs  \n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 7, 10, 11, 12, 13, 6,...  \n",
       "1  [130, 131, 132, 133, 51, 134, 135, 136, 137, 1...  \n",
       "2  [177, 35, 151, 178, 179, 180, 181, 168, 182, 5...  \n",
       "3  [310, 311, 312, 313, 314, 315, 316, 317, 318, ...  \n",
       "4  [126, 401, 402, 403, 404, 405, 70, 406, 407, 4...  \n",
       "5  [412, 413, 320, 414, 415, 416, 417, 418, 47, 4...  \n",
       "6  [453, 454, 151, 455, 456, 457, 458, 459, 460, ...  \n",
       "7  [168, 151, 381, 151, 638, 639, 451, 640, 47, 6...  \n",
       "8  [703, 704, 705, 706, 51, 707, 69, 217, 49, 708...  \n",
       "9  [735, 736, 389, 737, 738, 739, 740, 741, 742, ...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokenized_Sentence</th>\n      <th>Sentence_Idxs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[moron, girlfriend, conduct, ritual, resurrect...</td>\n      <td>[2, 3, 4, 5, 6, 7, 8, 9, 7, 10, 11, 12, 13, 6,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[pathetic, worse, bad, madefortv, movie, cant,...</td>\n      <td>[130, 131, 132, 133, 51, 134, 135, 136, 137, 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[go, see, film, honestly, jim, carrey, past, m...</td>\n      <td>[177, 35, 151, 178, 179, 180, 181, 168, 182, 5...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[wesley, snipe, james, dial, assassin, hire, a...</td>\n      <td>[310, 311, 312, 313, 314, 315, 316, 317, 318, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[good, rent, buy, original, watch, someone, gu...</td>\n      <td>[126, 401, 402, 403, 404, 405, 70, 406, 407, 4...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[must, accompanied, special, rating, warning, ...</td>\n      <td>[412, 413, 320, 414, 415, 416, 417, 418, 47, 4...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[remarkable, disturbing, film, truelife, sense...</td>\n      <td>[453, 454, 151, 455, 456, 457, 458, 459, 460, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[made, film, love, film, somebody, wacky, sens...</td>\n      <td>[168, 151, 381, 151, 638, 639, 451, 640, 47, 6...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[inexhaustible, hunger, basic, training, movie...</td>\n      <td>[703, 704, 705, 706, 51, 707, 69, 217, 49, 708...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[christopher, guest, need, worry, supreme, hol...</td>\n      <td>[735, 736, 389, 737, 738, 739, 740, 741, 742, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    '''Make a dictionary where the string representation \n",
    "        of a lexicon item can be retrieved \n",
    "        from its numerical index\n",
    "    '''\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "df['Sentence_Idxs'] = tokens_to_idxs(df['Tokenized_Sentence'], words_lexicon)\n",
    "df[['Tokenized_Sentence', 'Sentence_Idxs']][:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WORDS:\n [[    0     0     0 ...   128    47   129]\n [    0     0     0 ...   175   109   176]\n [    0     0     0 ...   308   174   309]\n ...\n [    0     0     0 ...  2729  1676  5065]\n [    0     0     0 ...  1990  2028   252]\n [    0     0     0 ...  5961 18896   126]]\nSHAPE: (1250, 577) \n\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in df['Sentence_Idxs']]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(df['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\\n\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1250, 577)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "train_padded_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from tensorflow.keras.layers import Embedding, GRU\n",
    "\n",
    "def create_model(seq_input_len, n_word_input_nodes, n_word_embedding_nodes,\n",
    "                  n_hidden_nodes, stateful=False, batch_size=20):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(shape=(None,))\n",
    "\n",
    "    #Layers 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True)(word_input) #mask_zero will ignore 0 padding\n",
    "\n",
    "    #Layer 4\n",
    "    hidden_layer = GRU(units=n_hidden_nodes)(word_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    hidden_layer=Dense(512,activation= 'relu')(hidden_layer)\n",
    "    #Layer 5\n",
    "    output_layer = Dense(units=10,activation='softmax')(hidden_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=word_input, outputs=output_layer)\n",
    "    model.compile(loss=\"mean_squared_error\",\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    #sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(words_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=300,\n",
    "                     n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 176s 3s/step - loss: 41.0839 - accuracy: 0.0648\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 177s 3s/step - loss: 41.0839 - accuracy: 0.0616\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 178s 3s/step - loss: 41.0839 - accuracy: 0.0792\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 170s 3s/step - loss: 41.0839 - accuracy: 0.0752\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 155s 2s/step - loss: 41.0839 - accuracy: 0.0832\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22298c2a610>"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=train_padded_words[:,1:],y=df.rating, batch_size=20, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-chosen",
   "metadata": {},
   "source": [
    "# 2. (evil) XOR Problem\n",
    "\n",
    "Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequence’s end. Test the two approaches below:\n",
    "\n",
    "### 2.1 \n",
    "\n",
    "Generate a dataset of random <=100,000 binary strings of equal length <= 50. Train the LSTM; what is the maximum length you can train up to with precisison?\n",
    "    \n",
    "\n",
    "### 2.2\n",
    "\n",
    "Generate a dataset of random <=200,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got the code from https://github.com/mitchellvitez/lstm-xor/blob/master/lstm_xor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying something for 2.2....\n",
    "\n",
    "arr2=np.random.randint(0,2,200000)\n",
    "\n",
    "\n",
    "i2=np.random.randint(0,51)\n",
    "i1=0\n",
    "L=[]\n",
    "y2=[]\n",
    "while i2<=len(arr2):\n",
    "    L.append([arr2[i1:i2].astype(str).tolist()])\n",
    "    y2.append(arr2[i1:i2].sum()%2)\n",
    "    i1=i2\n",
    "    i2+=np.random.randint(0,51)\n",
    "\n",
    "\n",
    "L.append([arr2[i1:i2].astype(str).tolist()])\n",
    "y2.append(arr2[i1:i2].sum()%2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape check: (100000, 50, 2) = (100000, 50, 2)\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6940 - accuracy: 0.5103\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6931 - accuracy: 0.5097\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6930 - accuracy: 0.5045\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6930 - accuracy: 0.4963\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6924 - accuracy: 0.4926\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6826 - accuracy: 0.5244\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.2450 - accuracy: 0.9570\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0770 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 1)             16        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50, 2)             4         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "randomly selected sequence: [0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1.]\n",
      "prediction: 0\n",
      "confidence: 93.78%\n",
      "actual: 0.0\n"
     ]
    }
   ],
   "source": [
    "# I got the code from \n",
    "#https://github.com/mitchellvitez/lstm-xor/blob/master/lstm_xor.py\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEQ_LEN = 50\n",
    "COUNT = 100000\n",
    "\n",
    "bin_pair = lambda x: [x, not(x)]\n",
    "training = np.array([[bin_pair(random.choice([0, 1])) for _ in range(SEQ_LEN)] for _ in range(COUNT)]).astype(float)\n",
    "target = np.array([[bin_pair(x) for x in np.cumsum(example[:,0]) % 2] for example in training]).astype(float)\n",
    "\n",
    "print('shape check:', training.shape, '=', target.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LEN, 2)))\n",
    "model.add(LSTM(1, return_sequences=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(training, target, epochs=10, batch_size=128)\n",
    "model.summary()\n",
    "\n",
    "predictions = model.predict(training)\n",
    "i = random.randint(0, COUNT)\n",
    "chance = predictions[i,-1,0]\n",
    "print('randomly selected sequence:', training[i,:,0])\n",
    "print('prediction:', int(chance > 0.5))\n",
    "print('confidence: {:0.2f}%'.format((chance if chance > 0.5 else 1 - chance) * 100))\n",
    "print('actual:', np.sum(training[i,:,0]) % 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0e04b1326f7adf898a3303c7cf3075ad0f4e0ffa89ed1c922eca04dd65938dc03",
   "display_name": "Python 3.8.8 64-bit ('myenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}